{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Creating a Transformer\nThe first thing we’ll need to do to initialize a BERT model is load a configuration object:","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from transformers import BertConfig, TFBertModel","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:22:28.288822Z","iopub.execute_input":"2021-06-30T04:22:28.289250Z","iopub.status.idle":"2021-06-30T04:22:34.871144Z","shell.execute_reply.started":"2021-06-30T04:22:28.289147Z","shell.execute_reply":"2021-06-30T04:22:34.870022Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"config=BertConfig()\nmodel=TFBertModel(config)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:23:03.288953Z","iopub.execute_input":"2021-06-30T04:23:03.289344Z","iopub.status.idle":"2021-06-30T04:23:03.463654Z","shell.execute_reply.started":"2021-06-30T04:23:03.289303Z","shell.execute_reply":"2021-06-30T04:23:03.462831Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#The configuration contains many attributes that are used to build the model:\nprint(config)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:23:24.798738Z","iopub.execute_input":"2021-06-30T04:23:24.799346Z","iopub.status.idle":"2021-06-30T04:23:24.806060Z","shell.execute_reply.started":"2021-06-30T04:23:24.799299Z","shell.execute_reply":"2021-06-30T04:23:24.804944Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.4.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- Creating a model from the default configuration initializes it with random values\nThe model can be used in this state, but it will output gibberish; it needs to be trained first. We could train the model from scratch on the task at hand,this would require a long time and a lot of data, and it would have a non-negligible environmental impact. To avoid unnecessary and duplicated effort, it’s imperative to be able to share and reuse models that have already been trained.","metadata":{}},{"cell_type":"markdown","source":"# Loading a Transformer model that is already trained ","metadata":{}},{"cell_type":"code","source":"from transformers import TFBertModel\nmodel=TFBertModel.from_pretrained('bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:26:55.308637Z","iopub.execute_input":"2021-06-30T04:26:55.308999Z","iopub.status.idle":"2021-06-30T04:27:47.919911Z","shell.execute_reply.started":"2021-06-30T04:26:55.308970Z","shell.execute_reply":"2021-06-30T04:27:47.918971Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec4732236905426f8228c3712f9175da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/527M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9946708766a54728909a5b98f967d342"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results.","metadata":{}},{"cell_type":"markdown","source":"# Saving methods\n- Saving a model is as easy as loading one — we use the save_pretrained method, which is analogous to the from_pretrained method:","metadata":{}},{"cell_type":"code","source":"model.save_pretrained('directory_on_my_computer')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:30:12.336536Z","iopub.execute_input":"2021-06-30T04:30:12.336930Z","iopub.status.idle":"2021-06-30T04:30:13.060430Z","shell.execute_reply.started":"2021-06-30T04:30:12.336896Z","shell.execute_reply":"2021-06-30T04:30:13.059582Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"- This saves two files to your disk:\n\nconfig.json, tf_model.h5\n- The tf_model.h5 file is known as the state dictionary; it contains all your model’s weights. The two files go hand in hand; the configuration is necessary to know your model’s architecture, while the model weights are your model’s parameters.","metadata":{}},{"cell_type":"markdown","source":"## Using a Transformer model for inference","metadata":{}},{"cell_type":"markdown","source":"- Transformer models can only process numbers — numbers that the tokenizer generates. But before we discuss tokenizers, let’s explore what inputs the model accepts.\n\nTokenizers can take care of casting the inputs to the appropriate framework’s tensors, but to help you understand what’s going on, we’ll take a quick look at what must be done before sending the inputs to the model.","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:35:27.817516Z","iopub.execute_input":"2021-06-30T04:35:27.818393Z","iopub.status.idle":"2021-06-30T04:35:27.827138Z","shell.execute_reply.started":"2021-06-30T04:35:27.818345Z","shell.execute_reply":"2021-06-30T04:35:27.825565Z"}}},{"cell_type":"code","source":"sequences = [\n  \"Hello!\",\n  \"Cool.\",\n  \"Nice!\"\n]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:36:06.836385Z","iopub.execute_input":"2021-06-30T04:36:06.836744Z","iopub.status.idle":"2021-06-30T04:36:06.840689Z","shell.execute_reply.started":"2021-06-30T04:36:06.836714Z","shell.execute_reply":"2021-06-30T04:36:06.839710Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"- The tokenizer converts these to vocabulary indices which are typically called input IDs. Each sequence is now a list of numbers! The resulting output is:","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:36:26.335587Z","iopub.execute_input":"2021-06-30T04:36:26.335952Z","iopub.status.idle":"2021-06-30T04:36:26.341145Z","shell.execute_reply.started":"2021-06-30T04:36:26.335922Z","shell.execute_reply":"2021-06-30T04:36:26.340181Z"}}},{"cell_type":"code","source":"encoded_sequences = [\n  [ 101, 7592,  999,  102],\n  [ 101, 4658, 1012,  102],\n  [ 101, 3835,  999,  102]\n]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:36:52.915102Z","iopub.execute_input":"2021-06-30T04:36:52.915659Z","iopub.status.idle":"2021-06-30T04:36:52.919981Z","shell.execute_reply.started":"2021-06-30T04:36:52.915626Z","shell.execute_reply":"2021-06-30T04:36:52.919258Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"- tensors only accept rectangular shapes (think matrices). This “array” is already of rectangular shape, so converting it to a tensor is easy:","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ninputs=tf.constant(encoded_sequences)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:37:33.276667Z","iopub.execute_input":"2021-06-30T04:37:33.277044Z","iopub.status.idle":"2021-06-30T04:37:33.282994Z","shell.execute_reply.started":"2021-06-30T04:37:33.277010Z","shell.execute_reply":"2021-06-30T04:37:33.281970Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Using the tensors as inputs to the model\n#Making use of the tensors with the model is extremely simple — we just call the model with the inputs:\noutput=model(inputs)\n#While the model accepts a lot of different arguments, only the input IDs are necessary","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:38:19.444666Z","iopub.execute_input":"2021-06-30T04:38:19.445037Z","iopub.status.idle":"2021-06-30T04:38:19.672594Z","shell.execute_reply.started":"2021-06-30T04:38:19.445007Z","shell.execute_reply":"2021-06-30T04:38:19.671287Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:38:27.037183Z","iopub.execute_input":"2021-06-30T04:38:27.037711Z","iopub.status.idle":"2021-06-30T04:38:27.050045Z","shell.execute_reply.started":"2021-06-30T04:38:27.037680Z","shell.execute_reply":"2021-06-30T04:38:27.048804Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor: shape=(3, 4, 768), dtype=float32, numpy=\narray([[[ 4.4495672e-01,  4.8276237e-01,  2.7797243e-01, ...,\n         -5.4032564e-02,  3.9393413e-01, -9.4770178e-02],\n        [ 2.4942866e-01, -4.4093004e-01,  8.1772351e-01, ...,\n         -3.1916550e-01,  2.2992221e-01, -4.1171834e-02],\n        [ 1.3667539e-01,  2.2517797e-01,  1.4502037e-01, ...,\n         -4.6914738e-02,  2.8224230e-01,  7.5565636e-02],\n        [ 1.1788867e+00,  1.6738546e-01, -1.8187107e-01, ...,\n          2.4671380e-01,  1.0440768e+00, -6.1966730e-03]],\n\n       [[ 3.6435878e-01,  3.2464243e-02,  2.0257650e-01, ...,\n          6.0110077e-02,  3.2451323e-01, -2.0995270e-02],\n        [ 7.1865964e-01, -4.8725191e-01,  5.1740408e-01, ...,\n         -4.4012007e-01,  1.4553063e-01, -3.7544742e-02],\n        [ 3.3223283e-01, -2.3270914e-01,  9.4876423e-02, ...,\n         -2.5268185e-01,  3.2171962e-01,  8.1109535e-04],\n        [ 1.2523220e+00,  3.5754380e-01, -5.1320337e-02, ...,\n         -3.7839833e-01,  1.0526474e+00, -5.6254822e-01]],\n\n       [[ 2.4042316e-01,  1.4717816e-01,  1.2110275e-01, ...,\n          7.6061025e-02,  3.3564475e-01,  2.8261721e-01],\n        [ 6.5700638e-01, -3.2786560e-01,  2.4967590e-01, ...,\n         -2.5919539e-01,  2.0174685e-01,  3.3275157e-01],\n        [ 2.0159647e-01,  1.5782665e-01,  9.8973550e-03, ...,\n         -3.8850513e-01,  4.1307491e-01,  3.9731964e-01],\n        [ 1.0174986e+00,  6.4386678e-01, -7.8146619e-01, ...,\n         -4.2109233e-01,  1.0925049e+00, -4.8455451e-02]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(3, 768), dtype=float32, numpy=\narray([[-0.6856276 ,  0.5262491 ,  0.99995273, ...,  0.9999873 ,\n        -0.6112343 ,  0.9970657 ],\n       [-0.6054583 ,  0.4997172 ,  0.9998191 , ...,  0.9999407 ,\n        -0.67532736,  0.97692657],\n       [-0.7701509 ,  0.5447418 ,  0.99994177, ...,  0.9999845 ,\n        -0.4654932 ,  0.9893902 ]], dtype=float32)>, hidden_states=None, attentions=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenizers","metadata":{}},{"cell_type":"markdown","source":"#### Word- Based","metadata":{}},{"cell_type":"markdown","source":"- The first type of tokenizer that comes to mind is word-based. It’s generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:\n\nThere are also variations of word tokenizers that have extra rules for punctuation. With this kind of tokenizer, we can end up with some pretty large “vocabularies,” where a vocabulary is defined by the total number of independent tokens that we have in our corpus.\n\nEach word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word.\n\nIf we want to completely cover a language with a word-based tokenizer, we’ll need to have an identifier for each word in the language, which will generate a huge amount of tokens. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we’d need to keep track of that many IDs. Furthermore, words like “dog” are represented differently from words like “dogs”, and the model will initially have no way of knowing that “dog” and “dogs” are similar: it will identify the two words as unrelated. The same applies to other similar words, like “run” and “running”, which the model will not see as being similar initially.\n\nFinally, we need a custom token to represent words that are not in our vocabulary. This is known as the “unknown” token, often represented as ”[UNK]” or ””. It’s generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn’t able to retrieve a sensible representation of a word and you’re losing information along the way. The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token.\n\nOne way to reduce the amount of unknown tokens is to go one level deeper, using a character-based tokenizer.","metadata":{}},{"cell_type":"code","source":"tokenized_text = \"Jim Henson was a puppeteer\".split()\nprint(tokenized_text)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:09:02.115950Z","iopub.execute_input":"2021-06-30T05:09:02.116343Z","iopub.status.idle":"2021-06-30T05:09:02.121921Z","shell.execute_reply.started":"2021-06-30T05:09:02.116312Z","shell.execute_reply":"2021-06-30T05:09:02.120888Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"['Jim', 'Henson', 'was', 'a', 'puppeteer']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Character Based Tokenizer","metadata":{}},{"cell_type":"markdown","source":"- Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:\n\nThe vocabulary is much smaller.\nThere are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.\nBut here too some questions arise concerning spaces and punctuation:\n\n\n\nThis approach isn’t perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, it’s less meaningful: each character doesn’t mean a lot on its own, whereas that is the case with words. However, this again differs according to the language; in Chinese, for example, each character carries more information than a character in a Latin language.\n\nAnother thing to consider is that we’ll end up with a very large amount of tokens to be processed by our model: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters.\n\nTo get the best of both worlds, we can use a third technique that combines the two approaches: subword tokenization.","metadata":{}},{"cell_type":"markdown","source":"## Subword tokenization\n- Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.\n\nFor instance, “annoyingly” might be considered a rare word and could be decomposed into “annoying” and “ly”. These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of “annoyingly” is kept by the composite meaning of “annoying” and “ly”.\n- These subwords end up providing a lot of semantic meaning: for instance, in the example above “tokenization” was split into “token” and “ization”, two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word). This allows us to have relatively good coverage with small vocabularies, and close to no unknown tokens.\n\nThis approach is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.\n- Unsurprisingly, there are many more techniques out there. To name a few:\n\nByte-level BPE, as used in GPT-2,\nWordPiece, as used in BERT,\nSentencePiece or Unigram, as used in several multilingual models.","metadata":{}},{"cell_type":"markdown","source":"## Loading and saving","metadata":{}},{"cell_type":"markdown","source":"- Loading and saving tokenizers is as simple as it is with models. Actually, it’s based on the same two methods: from_pretrained and save_pretrained. These methods will load or save the algorithm used by the tokenizer (a bit like the architecture of the model) as well as its vocabulary (a bit like the weights of the model).\n\nLoading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the BertTokenizer class:","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:23:37.349695Z","iopub.execute_input":"2021-06-30T05:23:37.350038Z","iopub.status.idle":"2021-06-30T05:23:37.358089Z","shell.execute_reply.started":"2021-06-30T05:23:37.350009Z","shell.execute_reply":"2021-06-30T05:23:37.356632Z"}}},{"cell_type":"code","source":"from transformers import BertTokenizer\ntokenizer=BertTokenizer.from_pretrained('bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:33:44.368698Z","iopub.execute_input":"2021-06-30T05:33:44.369056Z","iopub.status.idle":"2021-06-30T05:33:48.437723Z","shell.execute_reply.started":"2021-06-30T05:33:44.369028Z","shell.execute_reply":"2021-06-30T05:33:48.436647Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae69bf76c480490facc1f9a5cf1d9e08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a77e5965cd44f8f95afd766f48b2973"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22756b981cfd4d4e8763d274a9a9b8c8"}},"metadata":{}}]},{"cell_type":"markdown","source":"- Similar to TFAutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntoken=AutoTokenizer.from_pretrained('bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:37:27.948180Z","iopub.execute_input":"2021-06-30T05:37:27.948603Z","iopub.status.idle":"2021-06-30T05:37:31.157510Z","shell.execute_reply.started":"2021-06-30T05:37:27.948571Z","shell.execute_reply":"2021-06-30T05:37:31.156320Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"token('are you okay!')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:38:10.618561Z","iopub.execute_input":"2021-06-30T05:38:10.618915Z","iopub.status.idle":"2021-06-30T05:38:10.631534Z","shell.execute_reply.started":"2021-06-30T05:38:10.618885Z","shell.execute_reply":"2021-06-30T05:38:10.630492Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 1132, 1128, 3008, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"#Saving a tokenizer is identical to saving a model:\n\ntoken.save_pretrained(\"directory_on_my_computer\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:39:23.508997Z","iopub.execute_input":"2021-06-30T05:39:23.509411Z","iopub.status.idle":"2021-06-30T05:39:23.560477Z","shell.execute_reply.started":"2021-06-30T05:39:23.509375Z","shell.execute_reply":"2021-06-30T05:39:23.559433Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"('directory_on_my_computer/tokenizer_config.json',\n 'directory_on_my_computer/special_tokens_map.json',\n 'directory_on_my_computer/vocab.txt',\n 'directory_on_my_computer/added_tokens.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"# let’s see how the input_ids are generated. To do this, we’ll need to look at the intermediate methods of the tokenizer.\n- Translating text to numbers is known as encoding. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs.\n\nAs we’ve seen, the first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called tokens. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained.\n\nThe second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a vocabulary, which is the part we download when we instantiate it with the from_pretrained method. Again, we need to use the same vocabulary used when the model was pretrained.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:43:39.339731Z","iopub.execute_input":"2021-06-30T05:43:39.340104Z","iopub.status.idle":"2021-06-30T05:43:39.344350Z","shell.execute_reply.started":"2021-06-30T05:43:39.340075Z","shell.execute_reply":"2021-06-30T05:43:39.343251Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"token=AutoTokenizer.from_pretrained('bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:44:19.667846Z","iopub.execute_input":"2021-06-30T05:44:19.668246Z","iopub.status.idle":"2021-06-30T05:44:22.424555Z","shell.execute_reply.started":"2021-06-30T05:44:19.668195Z","shell.execute_reply":"2021-06-30T05:44:22.423396Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"seq=token.tokenize('you are amazing')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:44:55.849986Z","iopub.execute_input":"2021-06-30T05:44:55.850394Z","iopub.status.idle":"2021-06-30T05:44:55.854923Z","shell.execute_reply.started":"2021-06-30T05:44:55.850360Z","shell.execute_reply":"2021-06-30T05:44:55.853757Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"seq","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:44:58.688509Z","iopub.execute_input":"2021-06-30T05:44:58.688876Z","iopub.status.idle":"2021-06-30T05:44:58.694409Z","shell.execute_reply.started":"2021-06-30T05:44:58.688846Z","shell.execute_reply":"2021-06-30T05:44:58.693571Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"['you', 'are', 'amazing']"},"metadata":{}}]},{"cell_type":"code","source":"#From tokens to input IDs\n#The conversion to input IDs is handled by the convert_tokens_to_ids tokenizer method:\nids=token.convert_tokens_to_ids(seq)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:46:18.765624Z","iopub.execute_input":"2021-06-30T05:46:18.766004Z","iopub.status.idle":"2021-06-30T05:46:18.771550Z","shell.execute_reply.started":"2021-06-30T05:46:18.765970Z","shell.execute_reply":"2021-06-30T05:46:18.770390Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"ids","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:46:21.368736Z","iopub.execute_input":"2021-06-30T05:46:21.369259Z","iopub.status.idle":"2021-06-30T05:46:21.374589Z","shell.execute_reply.started":"2021-06-30T05:46:21.369225Z","shell.execute_reply":"2021-06-30T05:46:21.373751Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"[1128, 1132, 6929]"},"metadata":{}}]},{"cell_type":"code","source":"##decode\ndecoded_string = token.decode([1128, 1132, 6929])\nprint(decoded_string)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:47:07.759323Z","iopub.execute_input":"2021-06-30T05:47:07.759681Z","iopub.status.idle":"2021-06-30T05:47:07.764948Z","shell.execute_reply.started":"2021-06-30T05:47:07.759648Z","shell.execute_reply":"2021-06-30T05:47:07.763929Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"you are amazing\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\nsequences = [\n  \"I've been waiting for a HuggingFace course my whole life.\",\n  \"So have I!\"\n]\n\ntokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\noutput = model(**tokens)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:04:31.588175Z","iopub.execute_input":"2021-06-30T06:04:31.588615Z","iopub.status.idle":"2021-06-30T06:05:04.945397Z","shell.execute_reply.started":"2021-06-30T06:04:31.588580Z","shell.execute_reply":"2021-06-30T06:05:04.944284Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dedbd74a71784cc0a9a41ddc418a7d45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea0cfb801768430b8ac8c9b286b4b4ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0851d2c4a4c45b1b117b57d6d61eddc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0bac72b0b34c2580d3573a7daf1a20"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_93']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:05:52.737393Z","iopub.execute_input":"2021-06-30T06:05:52.737760Z","iopub.status.idle":"2021-06-30T06:05:52.745031Z","shell.execute_reply.started":"2021-06-30T06:05:52.737731Z","shell.execute_reply":"2021-06-30T06:05:52.744053Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-1.5606979,  1.6122825],\n       [-3.6183178,  3.9137495]], dtype=float32)>, hidden_states=None, attentions=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}