{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## pipeline example\nfrom transformers import pipeline\nsent=pipeline('sentiment-analysis')\nsent(['i hate this subject'])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-30T04:17:07.615709Z","iopub.execute_input":"2021-06-30T04:17:07.616091Z","iopub.status.idle":"2021-06-30T04:17:37.849268Z","shell.execute_reply.started":"2021-06-30T04:17:07.616002Z","shell.execute_reply":"2021-06-30T04:17:37.848459Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da896c780d5447449047c06e925d603f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d858ef0fe54c4cc8bfcd0d13da68d595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b523419d6d5480e95101c82fa4dad3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46cbab6553bf43f890334384b4aee245"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"[{'label': 'NEGATIVE', 'score': 0.9996384978294373}]"},"metadata":{}}]},{"cell_type":"code","source":"## splitting into steps\n#default checkpoint of the sentiment-analysis pipeline is distilbert-base-uncased-finetuned-sst-2-english \nfrom transformers import AutoTokenizer\ncheckpoint='distilbert-base-uncased-finetuned-sst-2-english'\ntokenizer=AutoTokenizer.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:17:37.850426Z","iopub.execute_input":"2021-06-30T04:17:37.850821Z","iopub.status.idle":"2021-06-30T04:17:43.307124Z","shell.execute_reply.started":"2021-06-30T04:17:37.850792Z","shell.execute_reply":"2021-06-30T04:17:43.306180Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Transformer models only accept tensors as input\n##All this preprocessing needs to be done in exactly the same way as when the model was pretrained\n##To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the return_tensors argument:\nraw_inputs = [\n    \"I've been waiting for a HuggingFace course my whole life.\", \n    \"I hate this so much!\",]\ninputs=tokenizer(raw_inputs,padding=True,truncation=True,return_tensors='tf')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:17:43.309060Z","iopub.execute_input":"2021-06-30T04:17:43.309540Z","iopub.status.idle":"2021-06-30T04:17:43.334131Z","shell.execute_reply.started":"2021-06-30T04:17:43.309495Z","shell.execute_reply":"2021-06-30T04:17:43.333289Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"There are many different architectures available in ðŸ¤— Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:\n\n*Model (retrieve the hidden states)\n*ForCausalLM\n*ForMaskedLM\n*ForMultipleChoice\n*ForQuestionAnswering\n*ForSequenceClassification\n*ForTokenClassification\nand others ðŸ¤—\nFor our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we wonâ€™t actually use the TFAutoModel class, but TFAutoModelForSequenceClassification:","metadata":{}},{"cell_type":"code","source":"from transformers import TFAutoModelForSequenceClassification\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:17:43.335328Z","iopub.execute_input":"2021-06-30T04:17:43.336580Z","iopub.status.idle":"2021-06-30T04:17:59.077458Z","shell.execute_reply.started":"2021-06-30T04:17:43.336539Z","shell.execute_reply":"2021-06-30T04:17:59.076425Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8218cb1025548f38ad26ee250950f4d"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n\nAll the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"outputs=model(inputs)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:17:59.078801Z","iopub.execute_input":"2021-06-30T04:17:59.079114Z","iopub.status.idle":"2021-06-30T04:17:59.208028Z","shell.execute_reply.started":"2021-06-30T04:17:59.079080Z","shell.execute_reply":"2021-06-30T04:17:59.207111Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(outputs.logits.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:17:59.209333Z","iopub.execute_input":"2021-06-30T04:17:59.209625Z","iopub.status.idle":"2021-06-30T04:17:59.215266Z","shell.execute_reply.started":"2021-06-30T04:17:59.209597Z","shell.execute_reply":"2021-06-30T04:17:59.214189Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(2, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"##postprocessing\noutputs.logits","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:17:59.216385Z","iopub.execute_input":"2021-06-30T04:17:59.216654Z","iopub.status.idle":"2021-06-30T04:17:59.229672Z","shell.execute_reply.started":"2021-06-30T04:17:59.216628Z","shell.execute_reply":"2021-06-30T04:17:59.228684Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-1.5606962,  1.6122814],\n       [ 4.169231 , -3.3464472]], dtype=float32)>"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\n\npredictions = tf.math.softmax(outputs.logits, axis=-1)\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:17:59.232994Z","iopub.execute_input":"2021-06-30T04:17:59.233510Z","iopub.status.idle":"2021-06-30T04:17:59.242189Z","shell.execute_reply.started":"2021-06-30T04:17:59.233434Z","shell.execute_reply":"2021-06-30T04:17:59.241128Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tf.Tensor(\n[[4.0195383e-02 9.5980465e-01]\n [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.id2label","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:17:59.250200Z","iopub.execute_input":"2021-06-30T04:17:59.250511Z","iopub.status.idle":"2021-06-30T04:17:59.257221Z","shell.execute_reply.started":"2021-06-30T04:17:59.250481Z","shell.execute_reply":"2021-06-30T04:17:59.256105Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{0: 'NEGATIVE', 1: 'POSITIVE'}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}